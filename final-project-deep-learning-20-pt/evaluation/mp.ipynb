{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M2177.003100 Deep Learning <br> Final Proejct: Text-guided to Image Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting your work:\n",
    "<font color=red>**DO NOT clear the MP score **</font> so that TAs can grade the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../miscc/config.py:151: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  yaml_cfg = edict(yaml.load(f))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys, os\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "\n",
    "sys.path.append('..')\n",
    "from evaluation.model import CNN_ENCODER, RNN_ENCODER\n",
    "from utils.data_utils import CUBDataset\n",
    "\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "device = torch.device('cpu' if not torch.cuda.is_available() else 'cuda')\n",
    "\n",
    "from miscc.config import cfg, cfg_from_file\n",
    "cfg_from_file('../cfg/eval_birds.yml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(x1, x2, dim=1, eps=1e-8):\n",
    "    \"\"\"\n",
    "    Returns cosine similarity between x1 and x2, computed along dim\n",
    "    \"\"\"\n",
    "    w12 = torch.sum(x1 * x2, dim)\n",
    "    w1 = torch.norm(x1, 2, dim)\n",
    "    w2 = torch.norm(x2, 2, dim)\n",
    "    return (w12 / (w1 * w2).clamp(min=eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load image encoder\n",
      "Load text encoder\n"
     ]
    }
   ],
   "source": [
    "image_encoder = CNN_ENCODER(256)\n",
    "state_dict = torch.load('./sim_models/bird/image_encoder.pth', map_location=lambda storage, loc: storage)\n",
    "image_encoder.load_state_dict(state_dict)\n",
    "for p in image_encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "print('Load image encoder')\n",
    "image_encoder.eval()\n",
    "\n",
    "# load the image encoder model to obtain the latent feature of the real caption\n",
    "text_encoder = RNN_ENCODER(5450, nhidden=256)\n",
    "state_dict = torch.load('./sim_models/bird/text_encoder.pth', map_location=lambda storage, loc: storage)\n",
    "text_encoder.load_state_dict(state_dict)\n",
    "for p in text_encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "print('Load text encoder')\n",
    "text_encoder.eval()\n",
    "\n",
    "image_encoder = image_encoder.to(device)\n",
    "text_encoder = text_encoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.current_dir:\n",
      "/home/ccw/DL_final/final-project-deep-learning-20-pt/evaluation\n",
      "\n",
      "self.data_dir:\n",
      "/home/ccw/DL_final/final-project-deep-learning-20-pt/data/birds\n",
      "\n",
      "self.image_dir:\n",
      "/home/ccw/DL_final/final-project-deep-learning-20-pt/data/birds/CUB-200-2011/images\n",
      "\n",
      "filepath /home/ccw/DL_final/final-project-deep-learning-20-pt/data/birds/captions.pickle\n",
      "Load from:  /home/ccw/DL_final/final-project-deep-learning-20-pt/data/birds/captions.pickle\n",
      "\ttest data directory:\n",
      "/home/ccw/DL_final/final-project-deep-learning-20-pt/data/birds/test\n",
      "\n",
      "\t# of test filenames:(2933,)\n",
      "\n",
      "\texample of filename of test image:001.Black_footed_Albatross/Black_Footed_Albatross_0046_18\n",
      "\n",
      "\texample of caption and its ids:\n",
      "['this', 'is', 'a', 'small', 'bird', 'that', 'has', 'a', 'brilliant', 'blue', 'color', 'on', 'it', 's', 'body', 'a', 'slightly', 'darker', 'blue', 'on', 'it', 's', 'head', 'a', 'teal', 'color', 'on', 'it', 's', 'wings', 'and', 'a', 'light', 'colored', 'beak']\n",
      "[18, 19, 1, 250, 2, 33, 13, 1, 853, 50, 37, 86, 53, 54, 15, 1, 178, 31, 50, 86, 53, 54, 25, 1, 1054, 37, 86, 53, 54, 17, 8, 1, 67, 89, 10]\n",
      "\n",
      "\t# of test captions:(29330,)\n",
      "\n",
      "\t# of test caption ids:(29330,)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ccw/anaconda3/envs/dl_final/lib/python3.6/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128))\n",
    "])\n",
    "\n",
    "test_dataset = CUBDataset(cfg.DATA_DIR, transform=transform, split='test', eval_mode=True)\n",
    "\n",
    "print(f'\\ttest data directory:\\n{test_dataset.split_dir}\\n')\n",
    "print(f'\\t# of test filenames:{test_dataset.filenames.shape}\\n')\n",
    "print(f'\\texample of filename of test image:{test_dataset.filenames[0]}\\n')\n",
    "print(f'\\texample of caption and its ids:\\n{test_dataset.captions[0]}\\n{test_dataset.captions_ids[0]}\\n')\n",
    "print(f'\\t# of test captions:{np.asarray(test_dataset.captions).shape}\\n')\n",
    "print(f'\\t# of test caption ids:{np.asarray(test_dataset.captions_ids).shape}\\n')\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=cfg.BATCH_SIZE,\n",
    "                                              drop_last=False, shuffle=False, num_workers=int(cfg.WORKERS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ccw/anaconda3/envs/dl_final/lib/python3.6/site-packages/torchvision/transforms/transforms.py:220: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  \"please use transforms.Resize instead.\")\n",
      "/home/ccw/anaconda3/envs/dl_final/lib/python3.6/site-packages/torch/nn/functional.py:2506: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# images for evaluation: 29330\n",
      "mean: 0.031076\n"
     ]
    }
   ],
   "source": [
    "MP_list = []\n",
    "DIFF_list = []\n",
    "SIM_list = []\n",
    "for data in test_dataloader:\n",
    "    imgs = data['img'][-1].to(device)\n",
    "    gen_imgs = data['gen_img'][-1].to(device)\n",
    "    captions = data['caps']\n",
    "    captions_lens = data['cap_len']\n",
    "    class_ids = data['cls_id']\n",
    "    keys = data['key']\n",
    "    sentence_idx = data['sent_ix']\n",
    "\n",
    "    sorted_cap_lens, sorted_cap_indices = torch.sort(captions_lens, 0, True)\n",
    "    captions = captions[sorted_cap_indices].squeeze()\n",
    "    if data['caps'].size(0) == 1:\n",
    "        captions = captions.unsqueeze(0)\n",
    "    class_ids = class_ids[sorted_cap_indices].numpy()\n",
    "    keys = [keys[i] for i in sorted_cap_indices.numpy()]\n",
    "\n",
    "    if cfg.CUDA:\n",
    "        captions = captions.to(device)\n",
    "        sorted_cap_lens = sorted_cap_lens.to(device)\n",
    "\n",
    "    hidden = text_encoder.init_hidden(captions.size(0))\n",
    "    _, sent_emb = text_encoder(captions, sorted_cap_lens, hidden)\n",
    "\n",
    "    _, sent_code = image_encoder(imgs)\n",
    "    _, gen_sent_code = image_encoder(gen_imgs)\n",
    "\n",
    "    sim = cosine_similarity(gen_sent_code, sent_emb)\n",
    "    l1 = torch.abs(imgs - gen_imgs)\n",
    "    diff = torch.mean(l1.view(l1.size(0), -1), dim=1)\n",
    "    mp = (1 - diff) * sim\n",
    "\n",
    "    MP_list.append(mp.detach().cpu().numpy())\n",
    "    DIFF_list.append(diff.detach().cpu().numpy())\n",
    "    SIM_list.append(sim.detach().cpu().numpy())\n",
    "\n",
    "MP_array = np.concatenate(MP_list, axis=0)\n",
    "DIFF_array = np.concatenate(DIFF_list, axis=0)\n",
    "SIM_array = np.concatenate(SIM_list, axis=0)\n",
    "print('# images for evaluation:', len(MP_array))\n",
    "print('mean:', \"%.6f\" % np.mean(MP_array))\n",
    "\n",
    "np.savez(cfg.MP_FILE, mp=MP_array, diff=DIFF_array, sim=SIM_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
